{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布式训练\n",
    "* 将数据与模型 分布到 单机多卡与多机多卡上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 设备数\n",
    "devie_count = torch.cuda.device_count()\n",
    "# 获取某个设备\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# 迁移模型或数据\n",
    "data=torch.ones((3,3))\n",
    "print(data.device)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "data_gpu = data.to(device)\n",
    "print(data_gpu.device)\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(3,3))\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单机多卡\n",
    "`torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)`\n",
    "* model 模型\n",
    "* device_ids GPU设备号\n",
    "* output_device 输出结果的设备，默认为0，第一块卡\n",
    "\n",
    "在模型推理过程中，数据被划分多个块，推送到不同的GPU，但是模型在每个GPU都会复制一份\n",
    "```\n",
    "class ASimpleNet(nn.Module):\n",
    "    def __init__(self, layers=3):\n",
    "        super(ASimpleNet, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(3, 3, bias=False) for i in range(layers)])\n",
    "    def forward(self, x):\n",
    "        print(\"forward batchsize is: {}\".format(x.size()[0]))\n",
    "        x = self.linears(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "        \n",
    "batch_size = 16\n",
    "inputs = torch.randn(batch_size, 3)\n",
    "labels = torch.randn(batch_size, 3)\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "net = ASimpleNet()\n",
    "net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "print(\"CUDA_VISIBLE_DEVICES :{}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "\n",
    "for epoch in range(1):\n",
    "    outputs = net(inputs)\n",
    "\n",
    "# Get:\n",
    "# CUDA_VISIBLE_DEVICES : 3, 2, 1, 0\n",
    "# forward batchsize is: 4\n",
    "# forward batchsize is: 4\n",
    "# forward batchsize is: 4\n",
    "# forward batchsize is: 4\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多机多卡\n",
    "### DP (dataparallel)\n",
    "* 单进程控制多GPU，将输入的一个batch数据分成了n份，分别送到对应的GPU进行计算。\n",
    "* 前向传播时，模型从主GPU复制到其他GPU，反向传播时，每个GPU上的梯度汇总到主GPU，主GPU求得梯度均值更新模型参数后，在把模型复制到其他GPU\n",
    "* 主GPU承担梯度汇总和模型更新，以及下发任务，负载高  \n",
    "\n",
    "### DDP (distributedDataParallel)\n",
    "* 多进程控制GPU\n",
    "* 数据加载采用分布式数据采集器，确保数据在各个进程没有重叠\n",
    "* 反向传播时，各个GPU梯度计算完成后，以广播形式将梯度汇总平均，然后各个进程在各自的GPU上进行梯度更新，确保各个GPU上的模型参数保持一致\n",
    "* 无需在GPU之间复制模型，DDP传输数据量更少，速度更快\n",
    "* 也适应于单机多卡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDP训练\n",
    "  * group  进程组,默认一个组\n",
    "  * world_size，全局进程个数\n",
    "  * rank 进程序号,用于进程间通信，表示进程优先级，rank=0表示主节点\n",
    "  * ![](images/distribution-train-2022-10-03-10-20-37.png)\n",
    "    * `torch.distributed.init_process_group(backend, init_method=None,, world_size=-1, rank=-1, group_name='')`\n",
    "    * 1初始化进程组 \n",
    "      * backend 通信所用后端，可以使nccl(gpu)或者gloo(cpu)\n",
    "      * init_method 指定进程的初始化方式，默认\"env://\",表示从环境变量初始化，也可以通过TCP方式共享文件系统\n",
    "      * world_size 执行训练的所有进程数，一般表示多少个节点\n",
    "      * rank 进程的编号，也是其优先级，表示当前节点的编号\n",
    "      * group_name 进程组名字\n",
    "    * 2模型并行化\n",
    "    * `torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0）`\n",
    "    * 3创建分布式数据采集器\n",
    "    * `train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)`    \n",
    "    * `data_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)`\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imagenet demo https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "\n",
    "if args.distributed: # 使用DDP\n",
    "     if args.dist_url == \"env://\" and args.rank == -1:\n",
    "         args.rank = int(os.environ[\"RANK\"])\n",
    "     if args.multiprocessing_distributed:\n",
    "         # For multiprocessing distributed training, rank needs to be the\n",
    "         # global rank among all the processes\n",
    "         args.rank = args.rank * ngpus_per_node + gpu\n",
    "     dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                             world_size=args.world_size, rank=args.rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('using CPU, this will be slow')\n",
    "elif args.distributed:\n",
    "    # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "    # should always set the single device scope, otherwise,\n",
    "    # DistributedDataParallel will use all available devices.\n",
    "    if args.gpu is not None:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model.cuda(args.gpu)\n",
    "        # When using a single GPU per process and per\n",
    "        # DistributedDataParallel, we need to divide the batch size\n",
    "        # ourselves based on the total number of GPUs we have\n",
    "        args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "        args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    else:\n",
    "        model.cuda()\n",
    "        # DistributedDataParallel will divide and allocate batch_size to all\n",
    "        # available GPUs if device_ids are not set\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "elif args.gpu is not None:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    model = model.cuda(args.gpu)\n",
    "else:\n",
    "    # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "    if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
    "        model.features = torch.nn.DataParallel(model.features)\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.distributed:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "else:\n",
    "    train_sampler = None\n",
    "# 在建立dataloader过程中，如果sampler不是none，那么shuffle参数不应该被设置\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
    "    num_workers=args.workers, pin_memory=True, sampler=train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 启动进程\n",
    "* 为每个机器节点上的gpu启动进程，\n",
    "* `torch.multiprocessing.spawn` 在一个节点启动该节点所有进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngpus_per_node 每个节点的gpu数量\n",
    " ngpus_per_node = torch.cuda.device_count()\n",
    " if args.multiprocessing_distributed:\n",
    "     # Since we have ngpus_per_node processes per node, the total world_size\n",
    "     # needs to be adjusted accordingly\n",
    "     args.world_size = ngpus_per_node * args.world_size\n",
    "     # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "     # main_worker process function\n",
    "     mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    " else:\n",
    "     # Simply call main_worker function\n",
    "     main_worker(args.gpu, ngpus_per_node, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 只在主节点保存模型\n",
    "if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "         and args.rank % ngpus_per_node == 0):\n",
    "     save_checkpoint({\n",
    "         'epoch': epoch + 1,\n",
    "         'arch': args.arch,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'best_acc1': best_acc1,\n",
    "         'optimizer' : optimizer.state_dict(),\n",
    "     }, is_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95dc1df185fd18e9076d56f64a89076aab4b6948bfab183b6c1ff3b5009283ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
